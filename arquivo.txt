from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

default_args = {
    'owner': 'Vinicius',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2025, 1, 1),
}

dag = DAG(
    dag_id='pipeline_produtos_vendas',
    default_args=default_args,
    description='Pipeline ETL de produtos e vendas',
    schedule='0 6 * * *',
    catchup=False,
    tags=['produtos', 'vendas', 'exercicio'],
)

### TASK 1 ###
def extract_produtos(**context):
    import pandas as pd
    import os
    import logging

    file_path = '/opt/airflow/data/produtos_loja.csv'
    if not os.path.exists(file_path):
        logging.error("Arquivo não encontrado!")
        return

    logging.info(f"Extraindo dados de: {file_path}")
    df = pd.read_csv(file_path)
    logging.info(f"Número de registros extraídos: {len(df)}")
    logging.info(f"Número de colunas: {df.shape[1]}")
    df.to_csv('/temp/dados_extraidos_produtos.csv', index=False)

### TASK 2 ###
def extract_vendas(**context):
    import pandas as pd
    import os
    import logging

    file_path = '/opt/airflow/data/vendas_produtos.csv'
    if not os.path.exists(file_path):
        logging.error("Arquivo não encontrado!")
        return

    logging.info(f"Extraindo dados de: {file_path}")
    df = pd.read_csv(file_path)
    logging.info(f"Número de registros extraídos: {len(df)}")
    logging.info(f"Número de colunas: {df.shape[1]}")
    df.to_csv('/temp/dados_extraidos_vendas.csv', index=False)

### TASK 3 ###
def transform_data(**context):
    import pandas as pd
    import logging

    logging.info("Iniciando transformação dos dados")
    df = pd.read_csv('/temp/dados_extraidos_produtos.csv')

    df['Fornecedor'] = df['Fornecedor'].fillna('Não informado').replace('', 'Não informado')
    df['Preco_Custo'] = pd.to_numeric(df['Preco_Custo'], errors='coerce')
    df['Preco_Custo'] = df['Preco_Custo'].fillna(df.groupby('Categoria')['Preco_Custo'].transform('mean'))
    df['Preco_Venda'] = pd.to_numeric(df['Preco_Venda'], errors='coerce')
    df['Preco_Venda'] = df['Preco_Venda'].fillna(df['Preco_Custo'] * 1.3)

    vendas_df = pd.read_csv('/opt/airflow/data/vendas_produtos.csv')
    vendas_df['Quantidade_Vendida'] = pd.to_numeric(vendas_df['Quantidade_Vendida'], errors='coerce').fillna(0)
    vendas_df['Preco_Venda'] = pd.to_numeric(vendas_df['Preco_Venda'], errors='coerce').fillna(0)
    vendas_agg = vendas_df.groupby('ID_Produto').agg({'Quantidade_Vendida': 'sum', 'Preco_Venda': 'mean'}).reset_index()
    vendas_agg.rename(columns={'Quantidade_Vendida': 'Total_Vendido', 'Preco_Venda': 'Media_Preco_Venda'}, inplace=True)
    df = df.merge(vendas_agg, on='ID_Produto', how='left')
    df['Total_Vendido'] = df['Total_Vendido'].fillna(0)
    df['Media_Preco_Venda'] = df['Media_Preco_Venda'].fillna(df['Preco_Venda'])
    df['Receita_Total'] = df['Total_Vendido'] * df['Media_Preco_Venda']

    df.to_csv('/temp/dados_transformados_produtos.csv', index=False)
    logging.info(f"Transformação concluída: {len(df)} registros processados")

### TASK 4 ###
create_tables = PostgresOperator(
    task_id='create_tables',
    postgres_conn_id='postgres_default',
    sql="""
    CREATE TABLE IF NOT EXISTS produtos_processados (
        ID_Produto VARCHAR(10),
        Nome VARCHAR(100),
        Categoria VARCHAR(50),
        Fornecedor VARCHAR(100),
        Preco_Custo DECIMAL(10,2),
        Preco_Venda DECIMAL(10,2),
        Total_Vendido INTEGER,
        Media_Preco_Venda DECIMAL(10,2),
        Receita_Total DECIMAL(12,2)
    );
    CREATE TABLE IF NOT EXISTS vendas_processadas (
        ID_Venda VARCHAR(10),
        ID_Produto VARCHAR(10),
        Quantidade_Vendida INTEGER,
        Preco_Venda DECIMAL(10,2),
        Data_Venda DATE,
        Canal_Venda VARCHAR(50)
    );
    CREATE TABLE IF NOT EXISTS relatorio_vendas AS
    SELECT v.ID_Venda, v.ID_Produto, p.Nome, p.Categoria,
           v.Quantidade_Vendida, v.Preco_Venda, v.Data_Venda, v.Canal_Venda,
           p.Total_Vendido, p.Receita_Total
    FROM vendas_processadas v
    LEFT JOIN produtos_processados p ON v.ID_Produto = p.ID_Produto
    WHERE 1=0;
    """,
    dag=dag,
)

### TASK 5 ###
def load_data(**context):
    import pandas as pd
    import logging
    from airflow.providers.postgres.hooks.postgres import PostgresHook

    logging.info("Iniciando carga dos dados no PostgreSQL")
    postgres_hook = PostgresHook(postgres_conn_id='postgres_default')
    engine = postgres_hook.get_sqlalchemy_engine()

    df_produtos = pd.read_csv('/temp/dados_transformados_produtos.csv')
    df_vendas = pd.read_csv('/opt/airflow/data/vendas_produtos.csv')

    df_produtos.to_sql('produtos_processados', engine, if_exists='replace', index=False, method='multi')
    df_vendas.to_sql('vendas_processadas', engine, if_exists='replace', index=False, method='multi')

    with engine.connect() as conn:
        conn.execute("""
            DELETE FROM relatorio_vendas;
            INSERT INTO relatorio_vendas
            SELECT v.ID_Venda, v.ID_Produto, p.Nome, p.Categoria,
                   v.Quantidade_Vendida, v.Preco_Venda, v.Data_Venda, v.Canal_Venda,
                   p.Total_Vendido, p.Receita_Total
            FROM vendas_processadas v
            LEFT JOIN produtos_processados p ON v.ID_Produto = p.ID_Produto;
        """)
        conn.commit()

    with engine.connect() as conn:
        total_registros = conn.execute("SELECT COUNT(*) FROM relatorio_vendas;").scalar()
        logging.info(f"Validação concluída: {total_registros} registros inseridos em relatorio_vendas")

    return f"Carga concluída com sucesso: {total_registros} registros inseridos em relatorio_vendas"

### TASK 6 ###
def generate_report(**context):
    import pandas as pd
    import logging
    from airflow.providers.postgres.hooks.postgres import PostgresHook

    logging.info("Iniciando geração do relatório de vendas")
    engine = PostgresHook(postgres_conn_id='postgres_default').get_sqlalchemy_engine()
    df_produtos = pd.read_sql("SELECT * FROM produtos_processados", engine)
    df_vendas = pd.read_sql("SELECT * FROM vendas_processadas", engine)
    df_relatorio = pd.read_sql("SELECT * FROM relatorio_vendas", engine)

    total_por_categoria = df_relatorio.groupby('Categoria')['Receita_Total'].sum().reset_index().rename(columns={'Receita_Total': 'Total_Vendas'})
    produto_mais_vendido = df_relatorio.groupby(['ID_Produto','Nome'])['Quantidade_Vendida'].sum().reset_index().sort_values('Quantidade_Vendida', ascending=False).head(1)
    canal_maior_receita = df_relatorio.groupby('Canal_Venda')['Receita_Total'].sum().reset_index().sort_values('Receita_Total', ascending=False).head(1)
    df_produtos['Margem_Lucro'] = ((df_produtos['Preco_Venda'] - df_produtos['Preco_Custo']) / df_produtos['Preco_Custo'])*100
    margem_media_categoria = df_produtos.groupby('Categoria')['Margem_Lucro'].mean().reset_index().rename(columns={'Margem_Lucro':'Margem_Lucro_Media'})

    logging.info("Consolidando relatório final")
    resumo = {
        "total_vendas_por_categoria": total_por_categoria.to_dict(orient='records'),
        "produto_mais_vendido": produto_mais_vendido.to_dict(orient='records'),
        "canal_maior_receita": canal_maior_receita.to_dict(orient='records'),
        "margem_media_por_categoria": margem_media_categoria.to_dict(orient='records'),
    }

    relatorio_path = '/temp/relatorio_final.csv'
    total_por_categoria.to_csv(relatorio_path, index=False)
    logging.info(f"Relatório gerado com sucesso e salvo em: {relatorio_path}")
    return resumo

# Criando tasks do PythonOperator
t1 = PythonOperator(task_id='extract_produtos', python_callable=extract_produtos, dag=dag)
t2 = PythonOperator(task_id='extract_vendas', python_callable=extract_vendas, dag=dag)
t3 = PythonOperator(task_id='transform_data', python_callable=transform_data, dag=dag)
t5 = PythonOperator(task_id='load_data', python_callable=load_data, dag=dag)
t6 = PythonOperator(task_id='generate_report', python_callable=generate_report, dag=dag)

# Orquestrando DAG
t1 >> t2 >> t3 >> create_tables >> t5 >> t6
